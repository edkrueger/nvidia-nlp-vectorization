{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f66311e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea5a01ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9cc4bb",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "648eb4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0687898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is the first document',\n",
       " 'this document is the second document',\n",
       " 'and this is the third one',\n",
       " 'is this the first document']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatter_pattern = re.compile(r\"[^\\w\\s]\")\n",
    "formatted_corpus = [\n",
    "    re.sub(formatter_pattern, \"\", document).lower() for document in corpus\n",
    "]\n",
    "formatted_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007483b6",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "070a61ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'is', 'the', 'first', 'document'],\n",
       " ['this', 'document', 'is', 'the', 'second', 'document'],\n",
       " ['and', 'this', 'is', 'the', 'third', 'one'],\n",
       " ['is', 'this', 'the', 'first', 'document']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_pattern = re.compile(r\"\\b\\w\\w+\\b\")\n",
    "tokenized_corpus = [\n",
    "    tokenizer_pattern.findall(document) for document in formatted_corpus\n",
    "]\n",
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81355520",
   "metadata": {},
   "source": [
    "### Vocabulary Alphabetized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4968056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary_alphabetized(tokenized_corpus):\n",
    "\n",
    "    unique_tokens = reduce(\n",
    "        lambda unique_tokens, tokenized_document: unique_tokens\n",
    "        | set(tokenized_document),\n",
    "        tokenized_corpus,\n",
    "        set(),\n",
    "    )\n",
    "\n",
    "    return {token: idx for idx, token in enumerate(sorted(unique_tokens))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cc14fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0,\n",
       " 'document': 1,\n",
       " 'first': 2,\n",
       " 'is': 3,\n",
       " 'one': 4,\n",
       " 'second': 5,\n",
       " 'the': 6,\n",
       " 'third': 7,\n",
       " 'this': 8}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = get_vocabulary_alphabetized(tokenized_corpus)\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db5ebac",
   "metadata": {},
   "source": [
    "### Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd2156e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_with_vocabulary(tokenized_corpus, vocabulary):\n",
    "\n",
    "    hashed_corpus = list()\n",
    "\n",
    "    for tokenized_document in tokenized_corpus:\n",
    "\n",
    "        hashed_document = list()\n",
    "\n",
    "        for token in tokenized_document:\n",
    "\n",
    "            hashed_token = vocabulary[token]\n",
    "            hashed_document.append(hashed_token)\n",
    "\n",
    "        hashed_corpus.append(hashed_document)\n",
    "\n",
    "    return hashed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0268a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 3, 6, 2, 1], [8, 1, 3, 6, 5, 1], [0, 8, 3, 6, 7, 4], [3, 8, 6, 2, 1]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashed_corpus = hash_with_vocabulary(tokenized_corpus, vocabulary)\n",
    "hashed_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c29a2ea",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2eb94fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(hashed_corpus, max_idx):\n",
    "\n",
    "    matrix = []\n",
    "\n",
    "    for hashed_document in hashed_corpus:\n",
    "\n",
    "        vector = [0 for _ in range(0, max_idx + 1)]\n",
    "\n",
    "        for hashed_token in hashed_document:\n",
    "\n",
    "            vector[hashed_token] += 1\n",
    "\n",
    "        matrix.append(vector)\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "750d2b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_idx = max(vocabulary.values())\n",
    "max_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e75d0466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       " [0, 2, 0, 1, 0, 1, 1, 0, 1],\n",
       " [1, 0, 0, 1, 1, 0, 1, 1, 1],\n",
       " [0, 1, 1, 1, 0, 0, 1, 0, 1]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = vectorize(hashed_corpus, max_idx)\n",
    "X"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
