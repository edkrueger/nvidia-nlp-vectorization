{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f79d6bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84c285d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994841ea",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5d5df72",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f46a15ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is the first document',\n",
       " 'this document is the second document',\n",
       " 'and this is the third one',\n",
       " 'is this the first document']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatter_pattern = re.compile(r\"[^\\w\\s]\")\n",
    "formatted_corpus = [\n",
    "    re.sub(formatter_pattern, \"\", document).lower() for document in corpus\n",
    "]\n",
    "formatted_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593d31c4",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e21b7ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'is', 'the', 'first', 'document'],\n",
       " ['this', 'document', 'is', 'the', 'second', 'document'],\n",
       " ['and', 'this', 'is', 'the', 'third', 'one'],\n",
       " ['is', 'this', 'the', 'first', 'document']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_pattern = re.compile(r\"\\b\\w\\w+\\b\")\n",
    "tokenized_corpus = [\n",
    "    tokenizer_pattern.findall(document) for document in formatted_corpus\n",
    "]\n",
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d753e799",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5752903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(tokenized_corpus):\n",
    "\n",
    "    vocabulary = dict()\n",
    "    idx = 0\n",
    "    for tokenized_document in tokenized_corpus:\n",
    "        for token in tokenized_document:\n",
    "            if token not in vocabulary:\n",
    "                vocabulary[token] = idx\n",
    "                idx += 1\n",
    "\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7099234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 0,\n",
       " 'is': 1,\n",
       " 'the': 2,\n",
       " 'first': 3,\n",
       " 'document': 4,\n",
       " 'second': 5,\n",
       " 'and': 6,\n",
       " 'third': 7,\n",
       " 'one': 8}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = get_vocabulary(tokenized_corpus)\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feb83ef",
   "metadata": {},
   "source": [
    "### Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df4a36fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_with_vocabulary(tokenized_corpus, vocabulary):\n",
    "\n",
    "    hashed_corpus = list()\n",
    "\n",
    "    for tokenized_document in tokenized_corpus:\n",
    "\n",
    "        hashed_document = list()\n",
    "\n",
    "        for token in tokenized_document:\n",
    "\n",
    "            hashed_token = vocabulary[token]\n",
    "            hashed_document.append(hashed_token)\n",
    "\n",
    "        hashed_corpus.append(hashed_document)\n",
    "\n",
    "    return hashed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc440382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4], [0, 4, 1, 2, 5, 4], [6, 0, 1, 2, 7, 8], [1, 0, 2, 3, 4]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashed_corpus = hash_with_vocabulary(tokenized_corpus, vocabulary)\n",
    "hashed_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc17ddc",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a639a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(hashed_corpus, max_idx):\n",
    "\n",
    "    matrix = []\n",
    "\n",
    "    for hashed_document in hashed_corpus:\n",
    "\n",
    "        vector = [0 for _ in range(0, max_idx + 1)]\n",
    "\n",
    "        for hashed_token in hashed_document:\n",
    "\n",
    "            vector[hashed_token] += 1\n",
    "\n",
    "        matrix.append(vector)\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44d6c1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_idx = max(vocabulary.values())\n",
    "max_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb0ddfaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       " [1, 1, 1, 0, 2, 1, 0, 0, 0],\n",
       " [1, 1, 1, 0, 0, 0, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = vectorize(hashed_corpus, max_idx)\n",
    "X"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
